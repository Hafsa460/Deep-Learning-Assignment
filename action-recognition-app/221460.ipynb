{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3fYR8oaoGW5",
        "outputId": "8fa5d197-a6b4-4881-acdb-efde46c6eb4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PNjWWWMAoNug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/train_FUll\""
      ],
      "metadata": {
        "id": "Yyt-h2wLoP1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "XpD8urShW9YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = sorted(os.listdir(DATASET_PATH))\n",
        "label_map = {action: idx for idx, action in enumerate(actions)}\n",
        "num_classes = len(actions)\n",
        "print(\"Total classes:\", num_classes)"
      ],
      "metadata": {
        "id": "s4JQNqeZW7Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "G6xU5yacoUQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, seq_len=3, max_images=75, transform=None, cache_images=False):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.seq_len = seq_len\n",
        "        self.cache_images = cache_images\n",
        "        self.cached_data = []\n",
        "\n",
        "        for action in sorted(os.listdir(root_dir)):\n",
        "            action_path = os.path.join(root_dir, action)\n",
        "            images = sorted([img for img in os.listdir(action_path) if img.endswith(\".jpg\")])[:max_images]\n",
        "\n",
        "            for i in range(0, max_images, seq_len):\n",
        "                seq_imgs = images[i:i+seq_len]\n",
        "                if len(seq_imgs) == seq_len:\n",
        "                    paths = [os.path.join(action_path, img) for img in seq_imgs]\n",
        "                    self.samples.append((paths, label_map[action]))\n",
        "\n",
        "        if self.cache_images:\n",
        "            print(\"Caching images in memory...\")\n",
        "            for paths, label in tqdm(self.samples):\n",
        "                frames = []\n",
        "                for p in paths:\n",
        "                    img = Image.open(p).convert(\"RGB\")\n",
        "                    img = self.transform(img)\n",
        "                    frames.append(img)\n",
        "                self.cached_data.append((torch.stack(frames), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.cache_images:\n",
        "            return self.cached_data[idx]\n",
        "        paths, label = self.samples[idx]\n",
        "        frames = []\n",
        "        for p in paths:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            img = self.transform(img)\n",
        "            frames.append(img)\n",
        "        return torch.stack(frames), label\n"
      ],
      "metadata": {
        "id": "xWR2Mdh1oihL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        base = models.mobilenet_v2(pretrained=True)\n",
        "        for param in base.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.cnn = base.features\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.lstm = nn.LSTM(input_size=1280, hidden_size=256, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B*T, C, H, W)\n",
        "        feat = self.cnn(x)\n",
        "        feat = self.pool(feat).view(B, T, -1)\n",
        "        out, _ = self.lstm(feat)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "model = CNN_LSTM(num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "WdDs7CeQW1GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_IMAGES_PER_CLASS = 75\n",
        "EPOCHS = 10\n",
        "\n",
        "dataset = ActionSequenceDataset(\n",
        "    DATASET_PATH,\n",
        "    seq_len=SEQ_LEN,\n",
        "    max_images=MAX_IMAGES_PER_CLASS,\n",
        "    transform=transform,\n",
        "    cache_images=False\n",
        ")\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "print(\"Total sequences:\", len(dataset))"
      ],
      "metadata": {
        "id": "6qkCUxALW4R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR7_MJWdpUbK",
        "outputId": "748a09af-52c7-44ad-b5a5-dfb0bec726e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2340218076.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for frames, labels in tqdm(loader):\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {epoch_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "E65Uow4jWugT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/drive/MyDrive/cnn_Lstm_model.pth\"\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(\"Model saved at:\", MODEL_PATH)"
      ],
      "metadata": {
        "id": "DmNu6TlOos3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}